{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "mount_file_id": "14W9Nfb2imRAVAaBrRLHM9POb5WGfUOmy",
      "authorship_tag": "ABX9TyM63FRkMR85m3kMsKABqMf1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuhao831068/Vgg11_for_Classify_CCTV_with_Gun_appearance/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Sx4hWtb7D3M_"
      },
      "outputs": [],
      "source": [
        "# define dataset\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, root, train, transform=None):\n",
        "    if train:\n",
        "      image_root = Path(root) / 'train'\n",
        "    else:\n",
        "      image_root = Path(root) / 'test'\n",
        "\n",
        "    # filter DS.Store\n",
        "    self.paths = [i for i in image_root.rglob('*') if i.is_file() and i.name != '.DS_Store']\n",
        "    self.transform = transform\n",
        "\n",
        "\n",
        "    with open(Path(root) / 'classnames.txt','r') as f:\n",
        "      lines = f.readlines()\n",
        "      classes = []\n",
        "      for line in lines:\n",
        "        stripped_line = line.strip()\n",
        "        classes.append(stripped_line)\n",
        "        self.classes = classes\n",
        "\n",
        "        # Read descriptions from descriptions.txt\n",
        "    self.descriptions = {}\n",
        "    with open(Path(root) / 'descriptions.txt', 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ', 1)\n",
        "            if len(parts) == 2:\n",
        "                filename, description = parts\n",
        "                self.descriptions[filename] = description\n",
        "\n",
        "            # Vectorize descriptions\n",
        "    self.vectorizer = TfidfVectorizer()\n",
        "    all_descriptions = list(self.descriptions.values())\n",
        "    self.vectorizer.fit(all_descriptions)\n",
        "    self.vectorized_descriptions = {k: self.vectorizer.transform([v]).toarray()[0] for k, v in self.descriptions.items()}\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.paths[index]).convert('RGB')\n",
        "    img_path = self.paths[index]\n",
        "    class_name = self.paths[index].parent.name\n",
        "    class_idx = self.classes.index(class_name)\n",
        "    img_name = img_path.name\n",
        "\n",
        "        # Get the description for the image\n",
        "    description = self.descriptions.get(img_name, \"No description available\")\n",
        "    description_vector = self.vectorized_descriptions.get(img_name, np.zeros(self.vectorizer.get_feature_names_out().shape[0]))\n",
        "    description_vector = torch.FloatTensor(description_vector)  # make sure the data type is Float\n",
        "\n",
        "    if self.transform:\n",
        "      return self.transform(img), class_idx, description, description_vector\n",
        "    else:\n",
        "      return img, class_idx, description, description_vector\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.paths)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import model\n",
        "import torchvision.models as models\n",
        "model = models.vgg11_bn(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cqckM6dmklye",
        "outputId": "d0727119-22f3-45e8-b54e-5b565c6c3d24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import weights and set transformer\n",
        "import torchvision.transforms as transforms\n",
        "weights = models.VGG11_BN_Weights.DEFAULT\n",
        "vgg11_bn_transforms = weights.transforms()"
      ],
      "metadata": {
        "id": "9cv8silrkpPQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(root = '/content/drive/MyDrive/final dataset-sr-224-ordered',\n",
        "                             train = True,\n",
        "                             transform=vgg11_bn_transforms)\n",
        "test_dataset = ImageDataset(root = '/content/drive/MyDrive/final dataset-sr-224-ordered',\n",
        "                            train = False,\n",
        "                            transform=vgg11_bn_transforms)"
      ],
      "metadata": {
        "id": "RQnX1kxwkq-r"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JB7ai85qSQMh",
        "outputId": "3da97027-25af-4ef0-dbd9-cebe35e8ec91"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-1.5699, -1.5357, -1.5185,  ...,  0.3652, -1.6555, -1.5185],\n",
              "          [-1.5528, -1.5699, -1.5528,  ...,  0.5536, -1.6384, -1.5699],\n",
              "          [-1.5014, -1.5528, -1.5357,  ...,  0.5707, -1.7069, -1.4500],\n",
              "          ...,\n",
              "          [-0.6965, -0.6794, -0.1314,  ...,  0.4679,  0.4508,  0.5536],\n",
              "          [-0.7308, -0.6623,  0.0569,  ...,  0.4166,  0.3652,  0.4679],\n",
              "          [-0.7650, -0.7308, -0.0287,  ...,  0.3994,  0.3994,  0.5022]],\n",
              " \n",
              "         [[-1.4580, -1.4230, -1.4055,  ...,  0.4853, -1.5455, -1.4055],\n",
              "          [-1.4405, -1.4580, -1.4405,  ...,  0.6779, -1.5630, -1.4930],\n",
              "          [-1.3880, -1.4405, -1.4405,  ...,  0.6429, -1.6681, -1.3704],\n",
              "          ...,\n",
              "          [-0.7227, -0.7227, -0.1450,  ...,  0.5553,  0.5028,  0.5903],\n",
              "          [-0.7052, -0.6352,  0.1176,  ...,  0.5028,  0.4153,  0.4853],\n",
              "          [-0.7227, -0.6527,  0.0826,  ...,  0.4678,  0.4503,  0.5203]],\n",
              " \n",
              "         [[-1.3339, -1.2990, -1.2816,  ...,  0.8099, -1.2467, -1.1073],\n",
              "          [-1.3164, -1.3339, -1.3164,  ...,  0.9842, -1.2467, -1.1770],\n",
              "          [-1.2641, -1.3164, -1.2990,  ...,  0.9494, -1.3861, -1.0898],\n",
              "          ...,\n",
              "          [-0.5321, -0.5147,  0.0605,  ...,  0.6879,  0.6356,  0.7402],\n",
              "          [-0.4798, -0.4101,  0.3393,  ...,  0.5834,  0.5311,  0.6008],\n",
              "          [-0.4798, -0.4101,  0.3393,  ...,  0.5485,  0.5659,  0.6356]]]),\n",
              " 1,\n",
              " 'a group of people walking down the street',\n",
              " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.3487, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5626, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.3017, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4161, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3397, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2655, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3344, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True) #shuffle\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=False)"
      ],
      "metadata": {
        "id": "Hv_wTMvyk6yC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, text_feature_size, num_classes):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "        vgg11 = models.vgg11_bn(weights=models.VGG11_BN_Weights.DEFAULT)\n",
        "        self.image_model = nn.Sequential(\n",
        "            vgg11.features,\n",
        "            nn.AdaptiveAvgPool2d((7, 7)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * 7 * 7, 2048),  # Assuming the VGG11 feature size\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.text_model = nn.Sequential(\n",
        "            nn.Linear(text_feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 + 2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, text_features):\n",
        "        img_features = self.image_model(images)\n",
        "        text_features = self.text_model(text_features)\n",
        "        combined_features = torch.cat((img_features, text_features), dim=1)\n",
        "        outputs = self.classifier(combined_features)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Wmq6zxLIqOUF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# make model\n",
        "text_feature_size = len(train_dataset.vectorizer.get_feature_names_out())\n",
        "num_classes = len(train_dataset.classes)\n",
        "model = MultiModalModel(text_feature_size, num_classes)\n",
        "\n",
        "# define cost function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "iKUD1zU9W35m"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "num_epochs = 15\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels, _, text_features in train_dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        text_features = text_features.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, text_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "    # verify\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels, _, text_features in test_dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            text_features = text_features.to(device)\n",
        "\n",
        "            outputs = model(images, text_features)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Validation Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob_g_RBXXUkb",
        "outputId": "e241dbcf-ce95-4a2f-d0b2-3d69b9f19cba"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: 0.8712\n",
            "Validation Accuracy: 50.00%\n",
            "Epoch 2/15, Loss: 0.6872\n",
            "Validation Accuracy: 63.00%\n",
            "Epoch 3/15, Loss: 0.6483\n",
            "Validation Accuracy: 50.00%\n",
            "Epoch 4/15, Loss: 0.6298\n",
            "Validation Accuracy: 71.00%\n",
            "Epoch 5/15, Loss: 0.5260\n",
            "Validation Accuracy: 72.00%\n",
            "Epoch 6/15, Loss: 0.4511\n",
            "Validation Accuracy: 71.00%\n",
            "Epoch 7/15, Loss: 0.4239\n",
            "Validation Accuracy: 73.00%\n",
            "Epoch 8/15, Loss: 0.3564\n",
            "Validation Accuracy: 71.00%\n",
            "Epoch 9/15, Loss: 0.3151\n",
            "Validation Accuracy: 73.00%\n",
            "Epoch 10/15, Loss: 0.3157\n",
            "Validation Accuracy: 72.00%\n",
            "Epoch 11/15, Loss: 0.2993\n",
            "Validation Accuracy: 70.00%\n",
            "Epoch 12/15, Loss: 0.2313\n",
            "Validation Accuracy: 70.00%\n",
            "Epoch 13/15, Loss: 0.1999\n",
            "Validation Accuracy: 71.00%\n",
            "Epoch 14/15, Loss: 0.1888\n",
            "Validation Accuracy: 69.00%\n",
            "Epoch 15/15, Loss: 0.2215\n",
            "Validation Accuracy: 63.00%\n",
            "Finished Training\n"
          ]
        }
      ]
    }
  ]
}