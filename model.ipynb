{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "mount_file_id": "14W9Nfb2imRAVAaBrRLHM9POb5WGfUOmy",
      "authorship_tag": "ABX9TyOUXmYlGfsnV0QKvW+2BSAV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuhao831068/Vgg11_for_Classify_CCTV_with_Gun_appearance/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sx4hWtb7D3M_"
      },
      "outputs": [],
      "source": [
        "# define dataset\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, root, train, transform=None):\n",
        "    if train:\n",
        "      image_root = Path(root) / 'train'\n",
        "    else:\n",
        "      image_root = Path(root) / 'test'\n",
        "\n",
        "    # filter DS.Store\n",
        "    self.paths = [i for i in image_root.rglob('*') if i.is_file() and i.name != '.DS_Store']\n",
        "    self.transform = transform\n",
        "\n",
        "\n",
        "    with open(Path(root) / 'classnames.txt','r') as f:\n",
        "      lines = f.readlines()\n",
        "      classes = []\n",
        "      for line in lines:\n",
        "        stripped_line = line.strip()\n",
        "        classes.append(stripped_line)\n",
        "        self.classes = classes\n",
        "\n",
        "        # Read descriptions from descriptions.txt\n",
        "    self.descriptions = {}\n",
        "    with open(Path(root) / 'descriptions.txt', 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ', 1)\n",
        "            if len(parts) == 2:\n",
        "                filename, description = parts\n",
        "                self.descriptions[filename] = description\n",
        "\n",
        "            # Vectorize descriptions\n",
        "    self.vectorizer = TfidfVectorizer()\n",
        "    all_descriptions = list(self.descriptions.values())\n",
        "    self.vectorizer.fit(all_descriptions)\n",
        "    self.vectorized_descriptions = {k: self.vectorizer.transform([v]).toarray()[0] for k, v in self.descriptions.items()}\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.paths[index]).convert('RGB')\n",
        "    img_path = self.paths[index]\n",
        "    class_name = self.paths[index].parent.name\n",
        "    class_idx = self.classes.index(class_name)\n",
        "    img_name = img_path.name\n",
        "\n",
        "        # Get the description for the image\n",
        "    description = self.descriptions.get(img_name, \"No description available\")\n",
        "    description_vector = self.vectorized_descriptions.get(img_name, np.zeros(self.vectorizer.get_feature_names_out().shape[0]))\n",
        "    description_vector = torch.FloatTensor(description_vector)  # make sure the data type is Float\n",
        "\n",
        "    if self.transform:\n",
        "      return self.transform(img), class_idx, description, description_vector\n",
        "    else:\n",
        "      return img, class_idx, description, description_vector\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.paths)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import model\n",
        "import torchvision.models as models\n",
        "model = models.vgg11_bn(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cqckM6dmklye",
        "outputId": "d2582bb9-cfb0-4486-8faa-ad9aa84d6c1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /root/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n",
            "100%|██████████| 507M/507M [00:02<00:00, 211MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # import weights and set transformer\n",
        "import torchvision.transforms as transforms\n",
        "weights = models.VGG11_BN_Weights.DEFAULT\n",
        "vgg11_bn_transforms = weights.transforms()"
      ],
      "metadata": {
        "id": "9cv8silrkpPQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet_transform = transforms.Compose([\n",
        "#     transforms.RandomResizedCrop(224),\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])"
      ],
      "metadata": {
        "id": "uh4FOf0GUiET"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(root = '/content/drive/MyDrive/final dataset-sr-224-ordered',\n",
        "                             train = True,\n",
        "                             transform=vgg11_bn_transforms)\n",
        "test_dataset = ImageDataset(root = '/content/drive/MyDrive/final dataset-sr-224-ordered',\n",
        "                            train = False,\n",
        "                            transform=vgg11_bn_transforms)"
      ],
      "metadata": {
        "id": "RQnX1kxwkq-r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JB7ai85qSQMh",
        "outputId": "3fdb3dc3-c4d4-4908-a321-0cefd81bf12e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-1.5699, -1.5357, -1.5185,  ...,  0.3652, -1.6555, -1.5185],\n",
              "          [-1.5528, -1.5699, -1.5528,  ...,  0.5536, -1.6384, -1.5699],\n",
              "          [-1.5014, -1.5528, -1.5357,  ...,  0.5707, -1.7069, -1.4500],\n",
              "          ...,\n",
              "          [-0.6965, -0.6794, -0.1314,  ...,  0.4679,  0.4508,  0.5536],\n",
              "          [-0.7308, -0.6623,  0.0569,  ...,  0.4166,  0.3652,  0.4679],\n",
              "          [-0.7650, -0.7308, -0.0287,  ...,  0.3994,  0.3994,  0.5022]],\n",
              " \n",
              "         [[-1.4580, -1.4230, -1.4055,  ...,  0.4853, -1.5455, -1.4055],\n",
              "          [-1.4405, -1.4580, -1.4405,  ...,  0.6779, -1.5630, -1.4930],\n",
              "          [-1.3880, -1.4405, -1.4405,  ...,  0.6429, -1.6681, -1.3704],\n",
              "          ...,\n",
              "          [-0.7227, -0.7227, -0.1450,  ...,  0.5553,  0.5028,  0.5903],\n",
              "          [-0.7052, -0.6352,  0.1176,  ...,  0.5028,  0.4153,  0.4853],\n",
              "          [-0.7227, -0.6527,  0.0826,  ...,  0.4678,  0.4503,  0.5203]],\n",
              " \n",
              "         [[-1.3339, -1.2990, -1.2816,  ...,  0.8099, -1.2467, -1.1073],\n",
              "          [-1.3164, -1.3339, -1.3164,  ...,  0.9842, -1.2467, -1.1770],\n",
              "          [-1.2641, -1.3164, -1.2990,  ...,  0.9494, -1.3861, -1.0898],\n",
              "          ...,\n",
              "          [-0.5321, -0.5147,  0.0605,  ...,  0.6879,  0.6356,  0.7402],\n",
              "          [-0.4798, -0.4101,  0.3393,  ...,  0.5834,  0.5311,  0.6008],\n",
              "          [-0.4798, -0.4101,  0.3393,  ...,  0.5485,  0.5659,  0.6356]]]),\n",
              " 1,\n",
              " 'a group of people walking down the street',\n",
              " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.3487, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5626, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.3017, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4161, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3397, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2655, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3344, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True) #shuffle\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=False)"
      ],
      "metadata": {
        "id": "Hv_wTMvyk6yC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, text_feature_size, num_classes):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "        vgg11 = models.vgg11_bn(weights=models.VGG11_BN_Weights.DEFAULT)\n",
        "        self.image_model = nn.Sequential(\n",
        "            vgg11.features,\n",
        "            nn.AdaptiveAvgPool2d((7, 7)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * 7 * 7, 2048),  # Assuming the VGG11 feature size\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.text_model = nn.Sequential(\n",
        "            nn.Linear(text_feature_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 + 2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, text_features):\n",
        "        img_features = self.image_model(images)\n",
        "        text_features = self.text_model(text_features)\n",
        "        combined_features = torch.cat((img_features, text_features), dim=1)\n",
        "        outputs = self.classifier(combined_features)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Wmq6zxLIqOUF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FNji7Vksd6Qj",
        "outputId": "da6ffa82-2497-4b3c-c0d6-d50acb09daca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['abandoned', 'air', 'airport', 'aisle', 'alleyway', 'an', 'and',\n",
              "       'another', 'apartment', 'are', 'around', 'at', 'attacked', 'away',\n",
              "       'back', 'background', 'backpack', 'backyard', 'bag', 'balloons',\n",
              "       'bank', 'bar', 'basketball', 'bathroom', 'being', 'bench', 'bike',\n",
              "       'billboard', 'black', 'block', 'blue', 'both', 'bottle', 'box',\n",
              "       'boxes', 'bricks', 'bucket', 'building', 'bus', 'by', 'camera',\n",
              "       'car', 'carried', 'carrying', 'cars', 'cart', 'cashier', 'cell',\n",
              "       'chair', 'chairs', 'child', 'circular', 'city', 'classroom',\n",
              "       'computer', 'corner', 'couch', 'counter', 'courtroom', 'crossing',\n",
              "       'crowd', 'crowded', 'dark', 'dashboard', 'day', 'dealership',\n",
              "       'desk', 'desks', 'display', 'dog', 'door', 'doors', 'down',\n",
              "       'dress', 'drinking', 'drinks', 'driver', 'driveway', 'driving',\n",
              "       'eating', 'elevator', 'empty', 'encampment', 'factory', 'falling',\n",
              "       'fence', 'fighting', 'flashlight', 'floor', 'footage', 'found',\n",
              "       'franc', 'frisbee', 'from', 'front', 'fruit', 'games', 'garage',\n",
              "       'garbage', 'gas', 'gathered', 'gloves', 'green', 'grocery',\n",
              "       'ground', 'group', 'gun', 'gym', 'hallway', 'hand', 'held', 'her',\n",
              "       'highway', 'him', 'his', 'holding', 'homeless', 'hoodie',\n",
              "       'hospital', 'hotel', 'house', 'image', 'in', 'inside', 'into',\n",
              "       'is', 'it', 'items', 'jacket', 'jail', 'jewelry', 'jumping',\n",
              "       'karate', 'kitchen', 'knife', 'laptop', 'large', 'laying',\n",
              "       'living', 'lobby', 'looking', 'lot', 'lots', 'luggage', 'machine',\n",
              "       'machines', 'mall', 'man', 'many', 'mask', 'masks', 'men',\n",
              "       'middle', 'mirror', 'moment', 'motorcycle', 'motorcycles',\n",
              "       'narrow', 'near', 'neighborhood', 'next', 'night', 'of', 'office',\n",
              "       'officers', 'on', 'one', 'other', 'out', 'outside', 'over',\n",
              "       'pants', 'parked', 'parking', 'path', 'pavement', 'people',\n",
              "       'person', 'photo', 'photos', 'pile', 'playing', 'pointing',\n",
              "       'police', 'pool', 'porch', 'prison', 'pub', 'pulled', 'pushed',\n",
              "       'racket', 'rear', 'red', 'residential', 'restaurant', 'riding',\n",
              "       'rifle', 'road', 'room', 'running', 'safety', 'san', 'school',\n",
              "       'scooter', 'screen', 'seat', 'seen', 'selection', 'several',\n",
              "       'shirt', 'shopping', 'shot', 'showing', 'shows', 'side', 'sides',\n",
              "       'sidewalk', 'sign', 'sitting', 'skateboard', 'skateboarding',\n",
              "       'snow', 'stairs', 'stairway', 'stand', 'standing', 'station',\n",
              "       'stealing', 'steering', 'stop', 'store', 'street', 'streets',\n",
              "       'students', 'suburban', 'subway', 'suit', 'suitcase', 'suits',\n",
              "       'supermarket', 'surveillance', 'suv', 'table', 'tables', 'taken',\n",
              "       'television', 'tennis', 'that', 'the', 'this', 'through', 'to',\n",
              "       'towed', 'train', 'tree', 'truck', 'tv', 'two', 'umbrellas',\n",
              "       'uniforms', 'van', 'vending', 'vest', 'vet', 'video', 'view',\n",
              "       'waiting', 'walking', 'walks', 'wall', 'warehouse', 'was',\n",
              "       'watching', 'wheel', 'while', 'white', 'wii', 'window', 'with',\n",
              "       'woman', 'women', 'working', 'yard', 'yellow'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# make model\n",
        "text_feature_size = len(train_dataset.vectorizer.get_feature_names_out())\n",
        "num_classes = len(train_dataset.classes)\n",
        "model = MultiModalModel(text_feature_size, num_classes)\n",
        "\n",
        "# # define cost function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n"
      ],
      "metadata": {
        "id": "iKUD1zU9W35m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "num_epochs = 15\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    running_loss = 0.0\n",
        "    for images, labels, _, text_features in train_dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        text_features = text_features.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, text_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "    train_accuracy = 100 * train_correct / train_total\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "    print(f'Train Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # test\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels, _, text_features in test_dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            text_features = text_features.to(device)\n",
        "\n",
        "            outputs = model(images, text_features)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_accuracy = 100 * test_correct / test_total\n",
        "        print(f'Validation Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob_g_RBXXUkb",
        "outputId": "5722c3ea-85be-4dff-d44e-20d61f89c661"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: 0.8380\n",
            "Train Accuracy: 47.50%\n",
            "Validation Accuracy: 50.00%\n",
            "Epoch 2/15, Loss: 0.6900\n",
            "Train Accuracy: 54.00%\n",
            "Validation Accuracy: 63.00%\n",
            "Epoch 3/15, Loss: 0.6732\n",
            "Train Accuracy: 61.75%\n",
            "Validation Accuracy: 60.00%\n",
            "Epoch 4/15, Loss: 0.5987\n",
            "Train Accuracy: 72.75%\n",
            "Validation Accuracy: 67.00%\n",
            "Epoch 5/15, Loss: 0.5088\n",
            "Train Accuracy: 76.75%\n",
            "Validation Accuracy: 76.00%\n",
            "Epoch 6/15, Loss: 0.4849\n",
            "Train Accuracy: 78.75%\n",
            "Validation Accuracy: 58.00%\n",
            "Epoch 7/15, Loss: 0.4713\n",
            "Train Accuracy: 78.50%\n",
            "Validation Accuracy: 76.00%\n",
            "Epoch 8/15, Loss: 0.4189\n",
            "Train Accuracy: 81.25%\n",
            "Validation Accuracy: 70.00%\n",
            "Epoch 9/15, Loss: 0.4274\n",
            "Train Accuracy: 81.50%\n",
            "Validation Accuracy: 61.00%\n",
            "Epoch 10/15, Loss: 0.4165\n",
            "Train Accuracy: 80.50%\n",
            "Validation Accuracy: 72.00%\n",
            "Epoch 11/15, Loss: 0.3450\n",
            "Train Accuracy: 85.25%\n",
            "Validation Accuracy: 71.00%\n",
            "Epoch 12/15, Loss: 0.3400\n",
            "Train Accuracy: 82.00%\n",
            "Validation Accuracy: 68.00%\n",
            "Epoch 13/15, Loss: 0.3244\n",
            "Train Accuracy: 85.25%\n",
            "Validation Accuracy: 70.00%\n",
            "Epoch 14/15, Loss: 0.2874\n",
            "Train Accuracy: 86.25%\n",
            "Validation Accuracy: 68.00%\n",
            "Epoch 15/15, Loss: 0.2821\n",
            "Train Accuracy: 87.75%\n",
            "Validation Accuracy: 73.00%\n",
            "Finished Training\n"
          ]
        }
      ]
    }
  ]
}